### Crime Rate Dataset

---
title: Crime Rate Dataset analysis
author: Patrick Cullinane, MA-541
date: "05/16/2022"
output: html_document
theme: lumen
bibliography: references.bib  
---

<style type="text/css">
  h1 {
  text-align: center;
}
  body{
   font-family: "Times New Roman", Times, serif;
}
</style>

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

\newpage


```{r}
library(dplyr)
library(ggplot2)
library(car)
library(ggpubr)
library(tidyverse)
library(rstatix)
library(ggstatsplot)
library(FSA)
library(boot)

setwd("C:/Users/Laura/Documents/code/MA-541")
df <- read.csv("Crime_R.csv")


#str(df)
```

### Chapter 1: Introduction

The Crime Rate dataset is comprised of data crime rate and associated variables collected in the United States at two different periods of time. The dataset is broken down into 27 columns and 47 rows. The dataset was developed by the University of Sheffield 

```{r}
dim(df)

names(df)
```

The first 13 columns consist of crime rate data and other measurements taken at a point in time, and the next set consists of the same data taken a decade later. Additionally there is a column called Southern which consists of a binary variable, 1 if a Southern state 0 otherwise. The Southern column applys to both sets of columns within the dataset. We are not told the exact date at which the data has been collected. Overall the data is a mixture of discrete, binary, continuous variables. 

```{r}
# split data into year 0 and year + 10 

df0 <- df %>%
  select(-ends_with('10'))
df10 <- df %>%
  select(ends_with('10'))

head(df0,2)
tail(df0,2)
```

### Chapter2: Experiment 1

The first question we will examine will be whether there is a relationship between Males per 1000 females and states classified as Southern. To accomplish this we will use 3 columns; Southern, Males, and Males10. As mentioned previously Southern is a binary variable while Males and Males10 are discrete variables. Males and Males10 both refer to the number of males per 1000 females in counted in a US State. 

### Chapter 2.1: Exploratory Data Analysis

To get an idea of the make-up of the data we look at the summary statistics.

```{r}
str(df[,c("Southern","Males","Males10")])
summary(df[,c("Southern","Males","Males10")])
```

Next let's look at how the data's shape using some plots. 

```{r}
df1 <- df[,c("Southern","Males")]
df2 <- df[,c("Southern","Males10")]
stem(df$Southern)
stem(df$Males)
stem(df$Males10)
```

We can see from the stem plots that we have more Southern states. We should note that although it is not explicity stated in the data what constitutes a Southern state the data seems to align with the US census bureau's definition of a Southern state. The US census counts 16 states in total as Southern States, which appears to align with our data. Alternatively we can see there are 31 "0" states which in total make 47 states counted in this dataset. We are not told what comprise the 31 non-Southern states. 

```{r}
sum(df$Southern == 1)
sum(df$Southern == 0)

boxplot(df[,c("Males","Males10")])
```

From the stem plots we can see that Males and Males10 both appear to be rightly skewed. We can examaine this in more detail using a histogram. 

```{r}
ggplot(df, aes(x=Males)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")

ggplot(df, aes(x=Males10)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")
```

More detailed examination with our histograms and density plots again show us that data may be skewed to the right. This will be important to note as we further examine whether there is a difference in means between groups of data. 

### Chapter 2.2: Testing

With this intuition we will examine using anova whether there is a significant difference in group means. To accomplish this we must first encode our the Southern column to match Males and Males10, and then stack the data frames on top of eachother. Next we will run an anova test on the data. 

Next we formulate the following hypothesis:
$$ H_0: \alpha_1 = \alpha_2 = \alpha_3 = \alpha_4 $$
$$ H_1: \alpha_l \neq \alpha_k $$
The null hypothesis states that all groups measured will be the same while the alternative hypothesis states there is a difference in atleast two of the groups. 

To proceed further we need to make four groups of data to compare. To do this we will map a new variable from Southern to Males10. We will stick with the current mapping of the Males and Southern column which is "1" if Southern otherwise "0". 

In the second group we will use "3" to denote a Southern state and "2" otherwise. In this way we will have 4 groups of Males. At this point we will have four groups: 0:Non-Southern Males, 1:Southern Males, 2:Non-Southern Males10, 3:Southern Males. To run the test we then stack the data on top of eachother to create one long column of Males and Southern variables. Additionally we establish that we will reject the null hypothesis at an alpha of 0.05.  

```{r}

df2$Southern <- ifelse(df2$Southern == 0, 2,3)
names(df2)[2] <- "Males"

df_stack <- rbind(df1,df2)
df_stack$Southern <- as.factor(df_stack$Southern)

aov_test <- aov(Males ~ Southern, data=df_stack)
summary(aov_test)
```

As we can see from our test there appears to be a signficant difference between atleast two of the groups as the p-value is below our threshold of 0.05. Our test does not tell us which groups are different so we need to perform further tests to gain more insight. 
 
Although it appears that we can proceed with post-hoc testing to compare groups from our previous examination of the underlying data's distribution we still need to check that the underlying assumption of normality is met.

To do this we will use both graphical methods and formally with the Shapiro-Wilk test. We will use the residuals of the anova test to test this assumption.

```{r}
par(mfrow = c(1,2))
hist(aov_test$residuals)
qqPlot(aov_test$residuals)
```

Graphically it appears that the residuals are not nornally distributed but we need to examine this more formally with a kruskal test. 

We formulate the null hypothesis that the data comes from a normal distribution and the alternative hypothesis that the data does not come from a normal distribution and we establish an alpha of 0.05. 


```{r}
kruskal.test(Males ~ Southern, data = df_stack)
```

From our test we can see that our p-value is below our establish alpha level so we can reject the null hypothesis that the data comes from a normal distribution. With this bit of information we will proceed with a non-parametric test. In this case we will use the Dunn test. We formulate the same hypothesis we used for the anova test: The null hypothesis that the groups are equal and the the alternative hypothesis that at least two of the groups are different. 

```{r}
dunnTest(Males ~ Southern,
  data = df_stack,
  method = "holm"
)

```

### Chapter 2.3: Results Discussion

After running our non-parametric tests we see that there is a significant difference between group 1 and 2 (p < 0.05). Where group 1 corresponds to Non-Southern States & Males/1000 Females and group 2 are Southern States Males/1000 Females 10 years later. Therefore we accept the alertnative hypothesis that there is a difference in group 1 and group 2. 

From the results it appears that there is a difference between Southern and Non-Southern Males/Females a decade apart although no evidence was found to show a difference within the same year. 

```{r}
ggbetweenstats(
  data = df_stack,
  x = "Southern",
  y = "Males",
  type = "Non-parametric", 
  plot.type = "box",
  pairwise.comparisons = TRUE,
  pairwise.display = "significant",
  centrality.plotting = FALSE,
  bf.message = FALSE
)

```


\newpage
### Chapter 3: Experiment 2

The next experiment deals with examining if a relationship exists between variables of the dataset and CrimeRate and if we can build a model to predict Crimerate. 

```{r}
stem(df$CrimeRate)
```

To get started we look at the year 0 data to build any intuition on relationships between CrimeRate and any other variables. 

```{r}
model <- lm(CrimeRate~.,data=df0)
summary(model)
```

We can see from running the linear model on the data that it appears some relationship exists between CrimeRate, Youth, Education, ExpenditureYear0, Wage, and BelowWage. We will take these five features and perform further investigation on them to see if how their structure. 

```{r}
features <- df[,c("CrimeRate","Youth","Education","ExpenditureYear0","Wage","BelowWage")]

str(features)
pairs(features)
```
Based on the structure of the features it appears they would benefit from some transformation. We will conduct further experiments on what features to keep/transform.  

### Chapter 3.1: Feature Engineering

The first experiment we will run is removing the weakest variable to see if we have any improvment on the R-squared value and if we can see if the model without education is significantly better using anova. 


```{r}
lm.fit <- lm(
  CrimeRate ~ 
    Education + Youth + Wage + BelowWage 
    + ExpenditureYear0, data=df0)

lm.fit2 <- lm(
  CrimeRate ~
    Youth + Wage + BelowWage
    + ExpenditureYear0, data=df0
)

summary(lm.fit)
summary(lm.fit2)

anova(lm.fit, lm.fit2)

```

We can see that the model without the Education feature does not perform significanlty better. So we will move on to comparing a two models where we have log transformed 4 features in the first model (Youth, Wage, BelowWage and ExpenditureYear0) and model 2 has the same data but this time we remove Wage. 


```{r}
lm.fit3 <- lm(
  CrimeRate ~ 
    + log10(Youth)
    + log10(Wage)
    + log10(BelowWage) 
    + log10(ExpenditureYear0), data=df0
)

lm.fit4 <- lm(
  CrimeRate ~ 
  + log10(Youth)
  + log10(BelowWage) 
  + log10(ExpenditureYear0), data=df0
)

summary(lm.fit3)
summary(lm.fit4)

anova(lm.fit3, lm.fit4)
```
Our R-squared has improved slightly from the non-log transformed models and it looks like the second model in this group performs signficanly better. With this in mind we will now use a validation set approach to estimate the test error rate. We will split our data up roughly in half in this case because we don't have many training examples to work with. We will be using MSE to test the effectiveness of our model.  

### Chapter 3.2: Model Building

```{r}

set.seed(1)
train <- sample(47,24)

lm.fit4 <- lm(
  CrimeRate ~ 
  + log10(Youth)
  + log10(BelowWage)
  + log10(ExpenditureYear0), data=df0, subset = train
)

mean((df$CrimeRate - predict(lm.fit4, df0))[-train]^2)
```
The MSE estimate for this approach is 360.78
```{r}
lm.fit4 <- lm(
  CrimeRate ~ 
  + poly(log10(Youth),2)
  + poly(log10(BelowWage),2)
  + poly(log10(ExpenditureYear0),2), data=df0, subset = train
)

mean((df$CrimeRate - predict(lm.fit4, df0))[-train]^2)
```
Adding the poly feature to our features reduces the MSE to 275.93. 

Our next approach is to see if a dimensionality reduction technique can improve our model even more. In this case we are using PCA along with the three features from the model above. 

```{r}
new_features <- df[,c("Youth","BelowWage","ExpenditureYear0")]
pca <- prcomp(features, scale. = TRUE)


lm_pca <- lm(df0$CrimeRate ~ pca$x[,1] + pca$x[,2], subset=train)
mean((df0$CrimeRate - predict(lm_pca, df0))[-train]^2)
```
PCA showed a signficant drop in MSE as compared to our previous attempts. However, to get a more robust idea of how our model performs across the whole dataset. 

Next we will use the caret package to create a model pipeline were we can combine our preprocessing techique (PCA), cross-validation, and our linear regression model. 

```{r}
df0_subset1 <- df[,c("CrimeRate","Youth","BelowWage","ExpenditureYear0")]
df0_subset2 <- df[,c("CrimeRate10","Youth10","BelowWage10","ExpenditureYear10")]

names(df0_subset2)[1] <- "CrimeRate"
names(df0_subset2)[2] <- "Youth"
names(df0_subset2)[3] <- "BelowWage"
names(df0_subset2)[4] <- "ExpenditureYear0"

full_df <- rbind(df0_subset1,df0_subset2)

```


```{r}
library(caret)

set.seed(12)
train_index <- sample(1:nrow(full_df), 0.7 * nrow(full_df))
X_train <- full_df[train_index, ]
X_test <- full_df[-train_index, ]
head(X_train)


fit2 <- train(
    CrimeRate ~
      Youth + BelowWage + ExpenditureYear0
    , data=X_train
    , method = "lm"
    , preProcess=c("pca")
    , trControl = trainControl(method = "cv")
)

fit2_pred <- predict(fit2, X_test)
fit2

postResample(pred = fit2_pred, obs = X_test$CrimeRate)
```
```{r}

mean(
  (X_test$CrimeRate - predict(
            fit2, X_test[,c("Youth","BelowWage","ExpenditureYear0")])
   )^2
)

```




Finally we try a boosted linear model:

```{r}

fit3 <- train(
    CrimeRate ~
      log10(Youth) + log10(BelowWage) + log10(ExpenditureYear0)
    , data=X_train
    , method = "BstLm"
    , preProcess=c("center", "scale", "YeoJohnson", "pca")
    , trControl = trainControl(method = "cv")
)

fit3_pred <- predict(fit3, X_test)
fit3

postResample(pred = fit3_pred, obs = X_test$CrimeRate)

mean(
  (X_test$CrimeRate - predict(
            fit3, X_test[,c("Youth","BelowWage","ExpenditureYear0")])
   )^2
)

```
\newpage
### Chapter 4: Conclusion

We started off by running some basic linear regression on the dataset to see if we could notice any correlations that we could quickly iterate on. We did see that some variables, mainly Youth, Wage/BelowWage, ExpenditureYear0, appeared to be better correlated with CrimeRate. From there we looked at how some basic transformations impacted the data. It appeared that log transformation did well. This was confirmed through comparing models using anova. 

After exploring some basic feature elimination/transformation we moved into modeling our data. We started out with a basic linear model and compared models using log transformation and quadratic polynomials. Ultimately PCA appeared to perform best and we moved into the final phase which involved creating a predictive pipeline using the caret library. 

One issued that we faced was the relatively small dataset size. To increase the predictive ability of the model we stacked the data from year0 on top of year10, doubling our training and test sizes. 

The first pipeline method used a simple combination of PCA and linear regression. 10-fold cross validation was applied to make sure we were not overfitting and we did a 50% training and test split. Our final model used the Boosted Linear Model from caret, with preprocessing steps that involved centered, scaled, Yeo-Johnson transformation, and pca. Tweaking the training set slighly we chose 60/40 as the train/test split as this produced the best results. 

Ultimatley we were unable to produce a model with that was able to have a high R^2 and low MSE value. This may be due to the limited dataset size. Future work can be conducted on finding similar datasets that have more data such as more than two years worth of data. 

\newpage

### References:

1. Katy Dobson, "Crime Rate Dataset" www.statstutor.ac.uk </br>
2. US Census, "Census Regions and Divisions of the United States" https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf </br>
3. Max Kuhn, "The Caret Package" https://topepo.github.io/caret/train-models-by-tag.html#linear-regression </br>
4. Antoine Soetewey, "Kruskal-Wallis test, or the nonparametric version of the ANOVA" </br>
5. Hadi Safari Katesari, Various handouts and statslabs from MA-541 </br>
6. Stack Overflow, "Can Kruskal-Wallis be used for discrete data?" https://stats.stackexchange.com/questions/497730/can-kruskal-wallis-be-used-for-discrete-data </br>
7. Stack Overflow, "Kruskal-Wallis and Dunn's" https://stats.stackexchange.com/questions/111050/kruskal-wallis-and-dunns </br>

